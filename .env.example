# LLM Configuration
LLM_PROVIDER="ollama"
LLM_BACKEND_URL="http://localhost:11434/v1" # For Ollama running in the same container, /v1 added for OpenAI compatibility
LLM_DEEP_THINK_MODEL="qwen3:0.6b"
LLM_QUICK_THINK_MODEL="qwen3:0.6b"
LLM_EMBEDDING_MODEL="nomic-embed-text"
OPENAI_API_KEY="ollama-key" # Optional, if you want to use OpenAI models or ollama models with OpenAI API compatibility

# Agent Configuration
MAX_DEBATE_ROUNDS="1"
ONLINE_TOOLS="False" # Set to True if you want to enable tools that access the internet

# Note: For local Docker Compose when Ollama runs on the host machine (not in container),
# you might use LLM_BACKEND_URL="http://host.docker.internal:11434/v1"
# The current docker-compose setup runs Ollama inside the app service.
